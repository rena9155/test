{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a target=\"_blank\" href=\"https://learning.constructor.org\"><img src=\"https://drive.google.com/uc?id=1McNxpNrSwfqu1w-QtlOmPSmfULvkkMQV\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "_____\n",
    "\n",
    "<center> <h1> Advanced Python: NumPy & Linear Algebra </h1> </center>\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "_____\n",
    "\n",
    "<center>Constructor Learning, 2022</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1XSmmvfolN3iGjk476hRhjKjX-3an7F6P\" width=\"700\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "In this first notebook, we will go through the essential concepts of linear algebra for machine and deep learning. Furthermore, we will show you some real-world applications in order to show you why it is important to know them and how they will help them on your journey to becoming a data scientist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version: 1.16.2\n",
      "Python Version: 3.7.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Numpy Version: 1.16.2')\n",
    "print('Python Version: 3.7.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to NumPy\n",
    "2. Matrix Multiplication\n",
    "3. Vector Norms\n",
    "4. Types of Matrices\n",
    "5. Transpose and Inverse\n",
    "6. Array Broadcasting & Vectorization\n",
    "7. Eigendecomposition\n",
    "8. Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is NumPy?**\n",
    "\n",
    "- Array-oriented computing\n",
    "- Efficiently implemented multi-dimensional arrays\n",
    "- Designed for scientific computation\n",
    "- Optimisation during interpretation obtained through vectorisation:\n",
    "\t- Replaces explicit loops with array expressions\n",
    "\t- 2 to 3 orders of magnitude faster than normal iterations\n",
    "\n",
    "**Difference to Python Lists**\n",
    "\n",
    "1. Arrays support vectorised operations, while lists don’t (more on that later).\n",
    "2. Once an array is created, you cannot change its size. You will have to create a new array or overwrite the existing one.\n",
    "3. Every array has one and only one data type. All items in it should be of that data type.\n",
    "4. An equivalent NumPy array occupies much less memory than a python list of lists.\n",
    "\n",
    "**Documentation**\n",
    "\n",
    "We encourage you to always have a look at the documentation of the libraries that you are using. There is an attribute for almost anything you want to do to make your life easier. Check out the numpy reference guide here: https://docs.scipy.org/doc/numpy/reference/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays\n",
    "\n",
    "__We can create a NumPy array by passing a python list to it and using ` np.array()`. In this case, python creates the array we can see on the right here:__\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1mcYLB7edbuTNAsDgg2Nls8DONMMfAE_6\" width=\"600\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list\n",
    "list_ = [0, 4, 24, 7]    \n",
    "\n",
    "# Creating a numpy array\n",
    "np_array = np.array(list_)    # you can also create the numpy array directly: np.array([0, 4, 24, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Array:  [ 0  4 24  7]\n",
      "Data Type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Numpy Array: ', np_array)\n",
    "print('Data Type: ', type(np_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Create a multi-dimensional numpy array (a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing & Slicing: Extract specific items from an array \n",
    "\n",
    "__Indexing numpy arrays are similar to indexing python lists.__\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1TlVHlGLvnAnKRIN0thQxxxciIVED9FhI\" width=\"600\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 5 7]\n",
      " [3 5 8]\n",
      " [5 8 6]]\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array([[1, 5, 7], [3, 5, 8], [5, 8, 6]])\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 8, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a row\n",
    "np_array[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one item\n",
    "np_array[0, 1]    # here we have a 2-dimensional array, thus two indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a range\n",
    "np_array[1, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 5, 8],\n",
       "       [5, 8, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select rows and columns\n",
    "np_array[1:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Select the first two items of the first two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping Arrays\n",
    "\n",
    "In your data science journey, you may find yourself needing to switch the dimensions of a certain matrix. This is often the case in machine learning applications where a certain model expects a certain shape for the inputs that is different from your dataset. NumPy’s `reshape()` method is useful in these cases. You just pass it the new dimensions you want for the matrix. You can pass -1 for a dimension and NumPy can infer the correct dimension based on your matrix:\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1CJd4Y3Gt0CKmt_jwTxhHVFdvnU35htuA\" width=\"700\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 5]\n",
      " [3 7 5 2]\n",
      " [9 4 6 4]]\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array([[1, 2, 3, 5], [3, 7, 5, 2], [9, 4, 6, 4]])\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 4)\n",
      "Number of Rows:  3\n",
      "Number of Columns:  4\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of an array\n",
    "print('Shape:', np_array.shape)\n",
    "print('Number of Rows: ', np_array.shape[0])\n",
    "print('Number of Columns: ', np_array.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 5]\n",
      " [3 7]\n",
      " [5 2]\n",
      " [9 4]\n",
      " [6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping 2D Array: From (3, 4) to (6, 2) => just always has to total 12\n",
    "np_array = np_array.reshape((6, 2))\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (12,)\n",
      "Array:  [1 2 3 5 3 7 5 2 9 4 6 4]\n"
     ]
    }
   ],
   "source": [
    "# Flatten an array from 2D to 1D\n",
    "np_array = np_array.ravel()\n",
    "print('Shape: ', np_array.shape)\n",
    "print('Array: ', np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of the last array is (12, ). This is a 1-dimensional array, which can cause trouble when using it in some deep learning frameworks. You can always reshape this array using np_array.reshape(12, 1). This doesn't change its form but is less error prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Convert a 1D array to a 2D array with 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10,)\n",
      "Array:  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "ex_array = np.arange(10)\n",
    "print('Shape:', ex_array.shape)\n",
    "print('Array: ', ex_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code in this cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Vector Multiplication\n",
    "\n",
    "A matrix and a vector can be multiplied together as long as the rule of matrix multiplication is observed. Specifically, that the number of columns in the matrix must equal the number of items in the vector. As with matrix multiplication, the operation can be written using the dot notation. Because the vector only has one column, the result is always a vector.\n",
    "\n",
    "__A key distinction to make with arithmetic is the case of matrix multiplication using the dot product. NumPy gives every matrix a `dot()` method we can use to carry-out dot product operations with other matrices:__\n",
    "\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1lS0j5IA1TE2Bx_WZ__uuLCn8KiVRZdCR\" width=\"900\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "Here a video explaining this concept from a machine learning perspective: https://www.youtube.com/watch?v=gPegoVYp64w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Shape: array (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print('Matrix A: ')\n",
    "print(A)\n",
    "print('Shape: array', A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector B: \n",
      "[0.5 0.5]\n",
      "Shape: vector (2,)\n"
     ]
    }
   ],
   "source": [
    "# define vector\n",
    "B = np.array([0.5, 0.5])\n",
    "print('Vector B: ')\n",
    "print(B)\n",
    "print(\"Shape: vector\", B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting Vector C: \n",
      "[1.5 3.5 5.5]\n",
      "Shape: A*B (3,)\n"
     ]
    }
   ],
   "source": [
    "# multiply\n",
    "print('Resulting Vector C: ')\n",
    "C = A.dot(B)\n",
    "print(C)\n",
    "print(\"Shape: A*B\", C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elements-Wise Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two matrices with the **same size** can be multiplied together, and this is often called **element-wise matrix** multiplication or the **Hadamard product**.\n",
    "\n",
    "As with element-wise subtraction and addition, element-wise multiplication involves the\n",
    "multiplication of elements from each parent matrix to calculate the values in the new matrix. In Python, you can use the * operator.\n",
    "\n",
    "Mathematical concept of Hadamard Product: https://en.wikipedia.org/wiki/Hadamard_product_(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 3]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "# Define first matrix\n",
    "A = np.array([\n",
    "[1, 3],\n",
    "[2, 5]])\n",
    "print('Matrix A: ')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B: \n",
      "[[0 1]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "# Define second matrix\n",
    "B = np.array([\n",
    "[0, 1],\n",
    "[3, 2]])\n",
    "print('Matrix B: ')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A * B: \n",
      "[[ 0  3]\n",
      " [ 6 10]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply matrices\n",
    "C = (A * B)\n",
    "print('Matrix A * B: ')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-Matrix Multiplication (Dot Product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices A and B is a third matrix C. In order for this product to be defined, A must have the same number of columns as B has rows. If A is of shape m × n and B is of shape n × p, then C is of shape m × p. This is written as follows\n",
    "\n",
    "$A_{mxn}.B_{nxp} = C_{mxp}$\n",
    "\n",
    "Here a video explaining this essential concept from a machine learning perspective: https://www.youtube.com/watch?v=_lrHXJRukMw\n",
    "\n",
    "Furthermore, here a video about matrix multiplication properties useful in this context: https://www.youtube.com/watch?v=c7GhnL2N--I\n",
    "\n",
    "Matrix-Matrix Multiplication, or the dot product, can be calculated by using **np.dot()** or the **@**-operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Define first matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print('Matrix A: ')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B: \n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Define second matrix\n",
    "B = np.array([\n",
    "[1, 2],\n",
    "[3, 4]])\n",
    "print('Matrix B: ')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Dot Product using np.dot: \n",
      "[[ 7 10]\n",
      " [15 22]\n",
      " [23 34]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply matrices\n",
    "C = A.dot(B)     # alternatively you could also write np.dot(A, B)\n",
    "print('Matrix Dot Product using np.dot: ')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Dot Product using @ operator: \n",
      "[[ 7 10]\n",
      " [15 22]\n",
      " [23 34]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply matrices with @ operator\n",
    "D = A @ B\n",
    "print('Matrix Dot Product using @ operator: ')\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vector Norms\n",
    "\n",
    "Calculating the size or length of a vector is often required either directly or as part of a broader vector or vector-matrix operation. The length of the vector is referred to as the vector norm or the vector’s magnitude.\n",
    "\n",
    "The length of the vector is always a positive number, except for a vector with zero values. It is calculated using some measure that summarizes the distance of the vector from the origin of the vector space. In classical X-Y plots, it is origin represents a place where both X and Y are zero.\n",
    "\n",
    "Both $L_1$ and $L_2$ norms are needed to understand **Lasso, Ridge and Elastic-net regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L<sup>1</sup> Norm\n",
    "\n",
    "$$\n",
    "\\|v\\|_{1}=\\left|a_{1}\\right|+\\left|a_{2}\\right|+\\left|a_{3}\\right|\n",
    "$$\n",
    "\n",
    "L1 Norm is the sum of the magnitudes of the vectors in a space. It is the most natural way of measure distance between vectors, that is the sum of absolute difference of the components of the vectors.\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1iDFg3GCxd9_6GxfCMQtvTgl6QVTxHlWG\" width=\"300\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "In several machine learning applications, it is important to discriminate between elements that are exactly zero and elements that are small but nonzero. In these cases, we turn to a function that grows at the same rate in all locations, but retains mathematical simplicity. The L<sup>1</sup> norm is often used in Machine learning as a regularization technique for feature reduction. We will look into this more during the Machine Learning week.\n",
    "\n",
    "Here is some informaiton about Norms\n",
    "https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a:\n",
      "[5 2 7]\n"
     ]
    }
   ],
   "source": [
    "# Define a vector\n",
    "a = np.array([5, 2, 7])\n",
    "print('Vector a:')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Norm of a:\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "a_l1 = np.linalg.norm(a, 1)\n",
    "print('L1 Norm of a:')\n",
    "print(a_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L<sup>2</sup> Norm\n",
    "\n",
    "\n",
    "$$\n",
    "\\|v\\|_{2}=\\sqrt{a_{1}^{2}+a_{2}^{2}+a_{3}^{2}}\n",
    "$$  \n",
    "\n",
    "The L<sup>2</sup> norm calculates the distance of the vector coordinate from the origin of the vector space. As such, it is also known as the **Euclidean norm** as it is calculated as the Euclidean distance from the origin. The result is a positive distance value.\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1uYTH-HlWr_IA0uEBmgcVqD2q3tPc10DV\" width=\"500\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "\n",
    "The L<sup>2</sup> norm is often used when fitting machine learning algorithms as a regularization method, e.g. a method to keep the coefficients of the model small and, in turn, the model less complex. By far, the L2 norm is more commonly used than other vector norms in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector a:\n",
      "[5 2 7]\n"
     ]
    }
   ],
   "source": [
    "# Define a vector\n",
    "a = np.array([5, 2, 7])\n",
    "print('Vector a:')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm of a:\n",
      "8.831760866327848\n"
     ]
    }
   ],
   "source": [
    "a_l2 = np.linalg.norm(a)\n",
    "print('L2 Norm of a:')\n",
    "print(a_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Types of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A square matrix is a matrix where the number of rows (n) is equivalent to the number of columns (m).\n",
    "\n",
    "Given that the number of rows and columns match, the dimensions are usually denoted as n, e.g. n × n. The size of the matrix is called the **order**, so an order 4 square matrix is 4 × 4. The vector of values along the diagonal of the matrix from the top left to the bottom right is called the main diagonal. Below is an example of an order 3 square matrix.\n",
    "\n",
    "$$\n",
    "M=\\left( \\begin{array}{lll}{1} & {2} & {3} \\\\ {1} & {2} & {3} \\\\ {1} & {2} & {3}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Square matrices are readily added and multiplied together and are the basis of many simple\n",
    "linear transformations, such as rotations (as in the rotations of images). One very common use case scenario if square matrices is **PCA**, which is performed on co-variance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diagonal matrix is one where values outside of the main diagonal have a zero value, where the main diagonal is taken from the top left of the matrix to the bottom right. A diagonal matrix is often denoted with the variable **D** and may be represented as a full matrix or as a vector of values on the main diagonal.\n",
    "\n",
    "$$\n",
    "\\; Matrix: \\; D=\\left( \\begin{array}{lll}{1} & {0} & {0} \\\\ {0} & {2} & {0} \\\\ {0} & {0} & {3}\\end{array}\\right)\\;\n",
    "\\; Vector: \\; d=\\left( \\begin{array}{l}{d_{1,1}} \\\\ {d_{2,2}} \\\\ {d_{3,3}}\\end{array}\\right)\\;\n",
    "\\; Scalar: \\; d=\\left( \\begin{array}{l}{1} \\\\ {2} \\\\ {3}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "A diagonal matrix does not have to be square. In the case of a rectangular matrix, the diagonal would cover the dimension with the smallest length. NumPy provides the function np.diag() that can create a diagonal matrix from an existing matrix, or transform a vector into a diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal Vector:\n",
      "[3 6 9]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([\n",
    "    [3, 6, 8],\n",
    "    [2, 6, 3],\n",
    "    [1, 5, 9]\n",
    "])\n",
    "\n",
    "d = np.diag(M)\n",
    "print('Diagonal Vector:')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal Matrix:\n",
      "[[3 0 0]\n",
      " [0 6 0]\n",
      " [0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "D = np.diag(d)\n",
    "print('Diagonal Matrix:')\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An identity matrix is a square matrix that does not change a vector when multiplied. The values of an identity matrix are known and are equal to one. It is a  **diagonal matrix** as well. All of the scalar values along the main diagonal (top-left to bottom-right) have the value one, while all other values are zero.\n",
    "\n",
    "An identity matrix is often represented using the notation I or with the dimensionality I<sup>n</sup>, where n is a subscript that indicates the dimensionality of the square identity matrix. \n",
    "\n",
    "Multiplying and divind with an Identity matrix has no impact on a matrix. These matrices are needed to help solve mathematical equations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrix:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "I = np.identity(3)\n",
    "print('Identity Matrix:')\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transpose, Inverse and Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't familiar with the inverse and transpose of matrices, watch this video: https://www.youtube.com/watch?v=7snro4M6ukk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A defined matrix can be transposed, which creates a new matrix with columns and rows flipped. This is denoted by the superscript $T$ next to the matrix A<sup>T</sup>. An invisible diagonal line can be drawn through the matrix from top left to bottom right on which the matrix can be flipped to give the transpose. __Simply put, the columns of A<sup>T</sup> are the rows of A.__\n",
    "\n",
    "NumPy arrays have a convenient property called `T` to get the transpose of a matrix:\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1hKRrM79b1poBVE9wM-cpA3BlvRMP8xyx\" width=\"500\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print('Matrix A:')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A Transposed:\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate transpose\n",
    "C = A.T\n",
    "print('Matrix A Transposed:')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1FzfaH1t26XjG69Nwo7V7FweikWjz8hMN\" width=\"800\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "Matrix inversion is a process that finds another matrix that when multiplied with the matrix, results in an **identity matrix**. **Gauss-Jordan elimination** is one way to obtain. matrix inverse. Fortunately for us, there is python that does it in one command. Getting matrix inverse requires a process of Given a matrix A, find matrix B, such that AB = I<sup>n</sup> or BA = I<sup>n</sup>. The operation of inverting a matrix is indicated by a −1 superscript next to the matrix; for example, A <sup>-1</sup>. The result of the operation is referred to as the inverse of the original matrix.\n",
    "\n",
    "Simply put, whatever A does, A<sup>-1</sup> undoes.\n",
    "\n",
    "A matrix can be inverted in NumPy using the np.linalg.inv() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrix\n",
    "A = np.array([\n",
    "[1.0, 2.0],\n",
    "[3.0, 4.0]])\n",
    "print('Matrix A:')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Matrix:\n",
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "# invert matrix\n",
    "B = np.linalg.inv(A)\n",
    "print('Inverted Matrix:')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Identity Matrix I:\n",
      "[[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# multiply A and B\n",
    "I = A.dot(B)\n",
    "print('The Identity Matrix I:')\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is the estimate of the number of linearly independent rows or columns in a matrix. An intuition for rank is to consider it the number of dimensions spanned by all of the vectors within a matrix. For example, a rank of 0 suggest all vectors span a point, a rank of 1 suggests all vectors span a line, a rank of 2 suggests all vectors span a two-dimensional plane.\n",
    "\n",
    "NumPy provides the *matrix rank()* function for calculating the rank of an array. It uses the SVD method to estimate the rank. You will learn more about this method in the following notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix v1: \n",
      "[[1 2 3]\n",
      " [4 6 2]]\n",
      "+++++++++++++++++++++++++++++\n",
      "Rank of matrix v1:  2\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[1,2,3], [4,6,2]])\n",
    "print('Matrix v1: ')\n",
    "print(v1)\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "vr1 = np.linalg.matrix_rank(v1)\n",
    "print('Rank of matrix v1: ', vr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix v2: \n",
      "[[1 2 3]\n",
      " [4 6 2]\n",
      " [4 1 9]]\n",
      "+++++++++++++++++++++++++++++\n",
      "Rank of matrix v2:  3\n"
     ]
    }
   ],
   "source": [
    "v2 = np.array([[1,2,3], [4,6,2], [4, 1, 9]])\n",
    "print('Matrix v2: ')\n",
    "print(v2)\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "vr2 = np.linalg.matrix_rank(v2)\n",
    "print('Rank of matrix v2: ', vr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, Rank is the number of linearly independent rows or columns in a matrix. \n",
    "In the example below row one is repeated so the rank stays the same as the for the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix v3: \n",
      "[[1 2 3]\n",
      " [4 6 2]\n",
      " [2 4 6]\n",
      " [1 2 3]]\n",
      "+++++++++++++++++++++++++++++\n",
      "Rank of matrix v2:  2\n"
     ]
    }
   ],
   "source": [
    "v3 = np.array([[1,2,3], [4,6,2], [2, 4, 6],[1,2,3]])\n",
    "print('Matrix v3: ')\n",
    "print(v3)\n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "vr3 = np.linalg.matrix_rank(v3)\n",
    "print('Rank of matrix v2: ', vr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Array Broadcasting & Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is the name given to the method that NumPy uses to allow array arithmetic between arrays with a different shape or size. Although the technique was developed for NumPy, it has also been adopted more broadly in other numerical computational libraries, such as Theano and TensorFlow. \n",
    "\n",
    "Broadcasting solves the problem of arithmetic between arrays of differing shapes by in effect replicating the smaller array along the last mismatched dimension.\n",
    "\n",
    "Detailed explanation of broadcasting: https://docs.scipy.org/doc/numpy/user/theory.broadcasting.html#array-broadcasting-in-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Broadcast scalar to one-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a:\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Define array\n",
    "a = np.array([1, 2, 3])\n",
    "print('Array a:')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar b:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# define scalar\n",
    "b = 2\n",
    "print('Scalar b:')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted addition:\n",
      "[3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# broadcast\n",
    "c = a + b\n",
    "print('Broadcasted addition:')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise: Broadcast scalar to two-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a scalar and a two-dimensional array\n",
    "2. Multiply them using broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Put your code in this cell\n",
    "# 1. Create a scalar\n",
    "\n",
    "# 2. Create a two-dimensional array\n",
    "\n",
    "# 3. Multiply them and print out the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=1K8zVos59iknyKwH6VVOmKsSvPu7JPcou\" width=\"500\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "\n",
    "Many calculations require to repeatedly do the same operations with all items in one or several sequences, e.g. multiplying two vectors a = [1, 2, 3, 4, 5] and b = [6, 7, 8, 9, 10]. This is usually implemented with a loop (e.g. for or while loop) where each item is treated one by one, e.g. 1 * 6, then 2 * 7, etc. Modern computers have special registers for such operations that allow to operate on several items at once. This means that a part of the data, say 4 items each, is loaded and multiplied simultaneously. \n",
    "\n",
    "For the mentioned example where both vectors have a size of 5, this means that instead of 5 operations, only 2 are necessary (one with the first 4 elements and one with the last “left over” element). With 12 items to be multiplied on each side we had 3 operations instead of 12, with 40 we had 10 and so on.\n",
    "\n",
    "\n",
    "\n",
    "Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 2498587.799500751\n",
      "Time Vectorized version:8.976459503173828ms\n",
      "\n",
      "Result: 2498587.799500683\n",
      "Time Non-Vectorized version (for-loop):5437.456369400024ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print('Result:', c)\n",
    "print('Time Vectorized version:' + str(1000*(toc-tic)) + 'ms')\n",
    "print()\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range (len(a)):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print('Result:', c)\n",
    "print('Time Non-Vectorized version (for-loop):' + str(1000*(toc-tic)) + 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the vectorized calculation is up to 600 times faster than the for-loop implementation. Thus, when working with large datasets (especially prevalent in deep learning), vectorization is key to work more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed explanation (1) and more examples (2), watch these videos:\n",
    "1. https://www.youtube.com/watch?v=qsIrQi0fzbY\n",
    "2. https://www.youtube.com/watch?v=pYWASRauTzs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Real-World Example: Computers execute code, not formulas: https://bit.ly/2VSIIcY *(read the question and the first answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Eigendecomposition\n",
    "\n",
    "The eigendecomposition is one form of matrix decomposition i.e. splitting a matrix into other matrices. Decomposing a matrix means that we want to find a product of matrices that is equal to the initial matrix. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues. \n",
    "\n",
    "**Matrices as linear transformations** \n",
    "\n",
    "Matrices can be applied to vectors to monitor what happens to a vector when a matrix is applied to it i.e. does the vector get longer or short, does the vector changes it's direction?\n",
    "You can think of matrices as linear transformations. Some matrices will rotate your space, others will rescale it etc. So when we apply a matrix to a vector, we end up with a transformed version of the vector. When we say that we \"apply\" the matrix to the vector it means that we calculate the dot product of the matrix with the vector. \n",
    "\n",
    "For a better understanding  of linear transformations, watch this video: https://www.youtube.com/watch?v=kYB8IZa5AuE  \n",
    "\n",
    "**The Determinant**  \n",
    "The determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix. Geometrically, it can be viewed as the volume scaling factor of the linear transformation described by the matrix. \n",
    "\n",
    "To gain an intuition of what the determinant is, watch this video: https://www.youtube.com/watch?v=Ip3X9LOh2dk&vl=en\n",
    "\n",
    "**Calculating the determinant**\n",
    "\n",
    "<center><b>2x2 Matrix</b></center>\n",
    "$$\n",
    "A=\\left[ \\begin{array}{ll}{a} & {b} \\\\ {c} & {d}\\end{array}\\right]\\\\|A|=a d-b c \n",
    "$$ \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><b>3x3 Matrix</b></center>\n",
    "$$\n",
    "A=\\left[ \\begin{array}{lll}{a} & {b} & {c} \\\\ {d} & {e} & {f} \\\\ {g} & {h} & {i}\\end{array}\\right]\\\\\n",
    "|A|=a(e i-f h)-b(d i-f g)+c(d h-e g)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "*Optional: If you want to practice the calculation of determinants, here are some exercises: https://bit.ly/2ZZuHcJ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B: \n",
      "[[ 6  1  1]\n",
      " [ 4 -2  5]\n",
      " [ 2  8  7]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([\n",
    "    [6, 1, 1],\n",
    "    [4, -2, 5],\n",
    "    [2, 8, 7]\n",
    "])\n",
    "\n",
    "print('Matrix B: ')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the determinant using NumPy: \n",
      "-306.0\n"
     ]
    }
   ],
   "source": [
    "print('Calculating the determinant using NumPy: ')\n",
    "det_B = np.linalg.det(B)\n",
    "print(det_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the determinant using the mathematical formula: \n",
      "-306\n"
     ]
    }
   ],
   "source": [
    "print('Calculating the determinant using the mathematical formula: ')\n",
    "print(6*(-2*7 - 5*8) - 1*(4*7 - 5*2) + 1*(4*8 - -2*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvectors and eigenvalues**  \n",
    "\n",
    "Imagine that the transformation of the initial vector gives us a new vector that has the exact same direction. The scale can be different but the direction is the same. Applying the matrix doesn’t change the direction of the vector. This special vector is called an eigenvector of the matrix. The goal in estimating eigenvectors and eigenvalues is to find all such vectors that only change in shape when a matrix transformation is applied to them.\n",
    "\n",
    "This means that v is a eigenvector of **A** if **v** and **Av** are in the same direction or to rephrase it if the vectors Av and v are parallel. The output vector is just a scaled version of the input vector. This scaling factor is λ which is called the eigenvalue of A:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} \\boldsymbol{v}=\\lambda \\boldsymbol{v}\n",
    "$$\n",
    "\n",
    "Eigenvectors are **unit vectors**, which means that their length or magnitude is equal to 1.0.  Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude.\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=13xkT8_jH2Hl4tVWhJ73tMPTbUFLDG0z7\" width=\"600\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
    "\n",
    "__Note: Multiplying these three matrices together, or combining the transformations represented by the matrices as we showed here, will result in the original matrix__\n",
    "\n",
    "For a comprehensive explanation of eigenvectors and eigenvalues, watch this video: https://www.youtube.com/watch?v=PFDu9oVAE-g:\n",
    "\n",
    "- Conceptual understanding: start at the beginning until 13:03\n",
    "- For an explanation of the computational ideas: start at 5:15 until 13:03\n",
    "\n",
    "Check out this blog post for another visual explanation: http://setosa.io/ev/eigenvectors-and-eigenvalues/  \n",
    "\n",
    "An in-depth example of how to calculate eigenvalues and eigenvectors can be found here: https://www.scss.tcd.ie/~dahyotr/CS1BA1/SolutionEigen.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find eigenvalues and eigenvectors in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrix\n",
    "A = np.array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print('Matrix A:')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: \n",
      "[ 1.61168440e+01 -1.11684397e+00 -9.75918483e-16]\n"
     ]
    }
   ],
   "source": [
    "# Eigendecomposition\n",
    "values, vectors = np.linalg.eig(A)\n",
    "print('Eigenvalues: ')\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors: \n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "print('Eigenvectors: ')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__\n",
    "\n",
    "The eigen vectors returned from the function is list of vectors columnwise, ie: [-0.23197069 -0.52532209 -0.8186735] is 1st eigen vector. You can understand it better if you check the documentation of the function https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html.\n",
    "\n",
    "- It is also possible to view documentation of function in jupyter if you press shift+tab while the cursor pointer is in the function brackets (...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing a matrix using eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the process and reconstruct the original matrix given only the eigenvectors and eigenvalues:  \n",
    "1. The list of eigenvectors must be taken together as a matrix, where each vector becomes a column.\n",
    "2. The eigenvalues need to be arranged into a diagonal matrix (np.diag()).\n",
    "3. Create the inverse of the eigenvector matrix (np.inv()).\n",
    "4. Multiply these elements together (np.dot()).\n",
    "\n",
    "Mathematically the equation $Av = \\lambda v$ can be rewritten in matrix form as \n",
    "\n",
    "$AQ = QV$\n",
    "\n",
    "To get A back let's multiply with the inverse of Q on both sides\n",
    "\n",
    "$AQQ^{-1}=QVQ^{-1}$\n",
    "\n",
    "$Q*Q^{-1} = I$\n",
    "\n",
    "$A=QVQ^{-1}$\n",
    "\n",
    "\n",
    "\n",
    "We will use the eigenvalues and eigenvectors of the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q - Eigenvector Matrix:\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create matrix from eigenvectors\n",
    "Q = vectors\n",
    "print('Q - Eigenvector Matrix:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L - Diagonal Eigenvalue Matrix:\n",
      "[[ 1.61168440e+01  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00 -1.11684397e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.75918483e-16]]\n"
     ]
    }
   ],
   "source": [
    "# 2. Create diagonal matrix from eigenvalues\n",
    "L = np.diag(values)\n",
    "print('L - Diagonal Eigenvalue Matrix:')\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R - Inverse Eigenvector Matrix:\n",
      "[[-0.48295226 -0.59340999 -0.70386772]\n",
      " [-0.91788599 -0.24901003  0.41986593]\n",
      " [ 0.40824829 -0.81649658  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Create inverse of eigenvectors matrix\n",
    "R = np.linalg.inv(Q)\n",
    "print('R - Inverse Eigenvector Matrix:')\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B - Reconstructed Matrix:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print('B - Reconstructed Matrix:')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Find all eigenvectors of the matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[ 4.  -5.   6. ]\n",
      " [ 7.  -8.   6. ]\n",
      " [ 1.5 -0.5 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [4, -5, 6],\n",
    "    [7, -8, 6],\n",
    "    [1.5, -0.5, -2],\n",
    "])\n",
    "\n",
    "print('Matrix A:')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the cell below for your calculations.\n",
    "- Add cells by going on insert--insert cell below or use the keyboard shortcuts (a => create cell above, b => create cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select all that apply:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A)\\:\\left[ \\begin{array}{c}{-3} \\\\ {-2} \\\\ {1}\\end{array}\\right]\\;\n",
    "B)\\:\\left[ \\begin{array}{c}{1 / \\sqrt{6}} \\\\ {-1 / \\sqrt{6}} \\\\ {2 / \\sqrt{6}}\\end{array}\\right]\\;\n",
    "C)\\:\\left[ \\begin{array}{l}{-3} \\\\ {-3} \\\\ {-1}\\end{array}\\right]\\;\n",
    "D)\\:\\left[ \\begin{array}{c}{-2 / \\sqrt{9}} \\\\ {-2 / \\sqrt{9}} \\\\ {1 / \\sqrt{9}}\\end{array}\\right]\\;\n",
    "E)\\:\\left[ \\begin{array}{c}{1 / 2} \\\\ {-1 / 2} \\\\ {-1}\\end{array}\\right]\\;\n",
    "F)\\:\\left[ \\begin{array}{c}{-1} \\\\ {1} \\\\ {-2}\\end{array}\\right]\\;\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code in this cell or create additional cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write \"Yes\" or \"No\" for every option. *To edit this cell, double-click on it.*     \n",
    " \n",
    "A)  \n",
    "B)    \n",
    "C)    \n",
    "D)    \n",
    "E)    \n",
    "F)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Reconstruct a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: \n",
      "[ 7.77200187 -0.77200187]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([7.77200187, -0.77200187])\n",
    "\n",
    "vectors = np.array([\n",
    "    [0.91430087, -0.50857499],\n",
    "    [0.40503571,  0.8610177]\n",
    "])\n",
    "\n",
    "print('Eigenvalues: ')\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors: \n",
      "[[ 0.91430087 -0.50857499]\n",
      " [ 0.40503571  0.8610177 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Eigenvectors: ')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. Create matrix from eigenvectors\n",
    "Q = None\n",
    "\n",
    "# 2. Create diagonal matrix from eigenvalues\n",
    "L = None\n",
    "\n",
    "# 3. Create inverse of eigenvectors matrix\n",
    "R = None\n",
    "\n",
    "# 4. Reconstruct the original matrix\n",
    "M = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your calculations\n",
    "print('Reconstructed Matrix: ')\n",
    "print(M)\n",
    "print(np.round(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Matrix: \n",
      "[[6 4]\n",
      " [3 1]]\n"
     ]
    }
   ],
   "source": [
    "# Remark: your solution might only approximate these numbers\n",
    "sol = np.array([\n",
    "    [6, 4],\n",
    "    [3, 1]\n",
    "])\n",
    "print('Reconstructed Matrix: ')\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "Real World Application - Visualize multi-dimensional data: http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes it more stable than other methods, such as the eigendecomposition. As such, it is often used in a wide array of applications including compressing, denoising, and data reduction.\n",
    "\n",
    "$$\n",
    "A=U \\cdot \\Sigma \\cdot V^{T}\n",
    "$$  \n",
    "\n",
    "Where $A_{mxn}$ is the real m × n matrix that we wish to decompose, $U_{mxm}$ is an m × m matrix, $Σ_{mxn}$ (represented by the uppercase Greek letter sigma) is an m × n diagonal matrix, and V<sup>T</sup> is the V transpose of an n × n matrix where T is a superscript.\n",
    "\n",
    "The diagonal values in the Σ matrix are known as the **singular values** of the original matrix A. The columns of the U matrix are called the left-singular vectors of A, and the columns of V are called the right-singular vectors of A. The SVD is calculated via iterative numerical methods (you can call them trial and error). We will not go into the details of these methods. Every rectangular matrix has a singular value decomposition, although the resulting matrices may contain complex numbers and the limitations of floating point arithmetic may cause some matrices to fail to decompose neatly.\n",
    "\n",
    "\n",
    "#### Reduced SVD\n",
    "\n",
    "In applications it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD. Data with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem.  The result is a matrix with a lower rank that is said to approximate the original matrix.\n",
    "\n",
    "To do this it is possible to perform an SVD operation on the original data and select the top _r_ largest singular values in $\\Sigma$ matrix. These columns can be selected from $\\Sigma$  and the rows selected from $V^T$. Which leads to the following representation:\n",
    "\n",
    "$$\n",
    "A= \\hat{U} \\cdot \\hat{\\Sigma}  \\cdot \\hat { V}^T\n",
    "$$  \n",
    "\n",
    "Where $A_{mxn}$ is the original m × n matrix, $ \\hat {U}_{mxr}$ is an m × r matrix, $ \\hat {Σ}_{rxr}$ is an r × r diagonal matrix, and $\\hat {V}^T$ is an r × n matrix\n",
    "\n",
    "------\n",
    "Watch this video in order to understand the basic concepts of reduced SVD: \n",
    "\n",
    "https://www.youtube.com/watch?v=P5mlg91as1c\n",
    "\n",
    "Real World Application - SVD for Dimensionality Reduction  \n",
    "\n",
    "https://www.youtube.com/watch?v=UyAfmAZU_WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Matrix A: \n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6],\n",
    "[1, 3]])\n",
    "print('Initial Matrix A: ')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U - Left Singular Vectors: \n",
      "[[-0.22064931  0.33409464 -0.41643003 -0.81626018]\n",
      " [-0.50077223 -0.03369825 -0.71517393  0.4864338 ]\n",
      " [-0.78089515 -0.40149114  0.44358886 -0.17954543]\n",
      " [-0.30123716  0.85208572  0.34400756  0.2546859 ]]\n"
     ]
    }
   ],
   "source": [
    "# Factorize\n",
    "U, s, V = np.linalg.svd(A)\n",
    "print('U - Left Singular Vectors: ')\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s - Singular Values: \n",
      "[9.98428124 1.14635424]\n"
     ]
    }
   ],
   "source": [
    "print('s - Singular Values: ')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V - Right Singular Vectors: \n",
      "[[-0.59380127 -0.80461174]\n",
      " [-0.80461174  0.59380127]]\n"
     ]
    }
   ],
   "source": [
    "print('V - Right Singular Vectors: ')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "Real-World Application - Dimensionality Reduction: https://www.youtube.com/watch?v=UyAfmAZU_WI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. LU-decomposition \n",
    "\n",
    "L U decomposition of a matrix is the factorization of a given square matrix into two triangular matrices, one upper triangular matrix and one lower triangular matrix, such that the product of these two matrices gives the original matrix.\n",
    "\n",
    "<center><a target=\"_blank\" ><img src=\"https://drive.google.com/uc?id=19E2N6rdqX9Q_B49iWuBCcO8LynxSafYa\" width=\"500\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Find the LU decomposition of the following matrix:__ \n",
    " \\begin{bmatrix}\n",
    "    1     &  2   & 4  \\\\\n",
    "    3     &  8   & 14  \\\\\n",
    "    2     &  6   & 13  \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "- First perform the calculation manually on paper.\n",
    "- Check the solution in Python by using the dot-product (L.dot(U))\n",
    "\n",
    "On pivoting for LU factorization:\n",
    "http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-reid-LU-pivoting.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "[[ 1.  2.  4.]\n",
      " [ 3.  8. 14.]\n",
      " [ 2.  6. 13.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "A_ = L.dot(U)\n",
    "print(\"A: \\n{}\\n\".format(A_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you learned the relevant basics of linear algebra and how to implement these concepts in Python, specifically in NumPy. Further crucial concepts that every aspiring data scientist needs to know like eigendecomposition and singular value decomposition were introduced, as well as their real-world applications. The learning objectives of the pre-work is that you understand various mathematical concepts related to machine learning algorithms. So make sure that you have a solid understanding of them. If you need more practice or a more theoretical foundation, head over to the materials and math exercises in our github repository. \n",
    "\n",
    "Do not neglect the mathematics of machine learning! Understanding these concepts will give you a competitive advantage and you will learn much quicker during the whole program. \n",
    "\n",
    "In the second notebook of this pre-work series, we will dive in the matrix calculus that you need for machine and deep learning. Take your time and when your ready head over there to take the next step in your journey to becoming a data scientist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
